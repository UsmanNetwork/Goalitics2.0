import pandas as pd
from sklearn.model_selection import StratifiedKFold, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.metrics import accuracy_score, f1_score, make_scorer

# --- Recreate Data and Preprocessor ---
# NOTE: Replace 'epl_with_agent_features.csv' with the path to your file,
# e.g., '/content/drive/MyDrive/ML_Project/epl_final.csv'
FILE_PATH = 'epl_with_agent_features.csv'
try:
    df = pd.read_csv(FILE_PATH)
except FileNotFoundError:
    print(f"Error: Training data not found at '{FILE_PATH}'. Please update the path.")
    exit()


# Target Encoding
le = LabelEncoder()
df['Target'] = le.fit_transform(df['FullTimeResult'])

# Feature Selection
numerical_features = ['HomeShots', 'AwayShots', 'HomeShotsOnTarget', 'AwayShotsOnTarget',
                      'HomeCorners', 'AwayCorners', 'HomeFouls', 'AwayFouls',
                      'HomeYellowCards', 'AwayYellowCards', 'HomeRedCards', 'AwayRedCards',
                      'Home_Optimal_Form_Score', 'Away_Optimal_Form_Score', 'Optimal_Form_Differential']
categorical_features = ['HomeTeam', 'AwayTeam']

X = df[numerical_features + categorical_features]
y = df['Target']

# Time-Series Train-Test Split (80/20 split)
split_point = int(0.8 * len(X))
X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]

# Preprocessing Pipeline Definition
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        # sparse_output=False is the correct modern parameter name for dense output
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ],
    remainder='passthrough'
)

# --- DELIVERABLE 4: HYPERPARAMETER TUNING SETUP ---

# 1. Define the XGBoost Classifier
xgb_classifier = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    eval_metric='mlogloss',
    random_state=42,
    n_estimators=100
    # REMOVED: use_label_encoder=False for Sklearn 1.4+ compatibility
)

# 2. Define the Imblearn Pipeline (Preprocessing -> SMOTE -> XGBoost)
smote_sampler = SMOTE(random_state=42)
pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('sampler', smote_sampler),
    ('classifier', xgb_classifier)
])

# 3. Define the Hyperparameter Grid to Search
param_grid = {
    'classifier__max_depth': [3, 5, 7],
    'classifier__learning_rate': [0.1, 0.05],
    'classifier__gamma': [0, 0.5]
}

# 4. Define the Scoring Metric
scorer = make_scorer(f1_score, average='macro', zero_division=0)

# 5. Define Cross-Validation Strategy
# TimeSeriesSplit is used to respect the chronological order of the data.
time_split = TimeSeriesSplit(n_splits=3)


# 6. Initialize Grid Search
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring=scorer,
    cv=time_split,
    verbose=1,
    n_jobs=-1
)

print("\n--- Starting Grid Search for Hyperparameter Tuning (Optimizing Macro F1-Score) ---")
grid_search.fit(X_train, y_train)

# --- 7. Final Evaluation on the Test Set ---
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calculate Final Metrics
final_accuracy = accuracy_score(y_test, y_pred)
final_f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)

print("\n\n--- Final Optimized XGBoost Model Performance ---")
print(f"Best Parameters Found: {grid_search.best_params_}")
print("-" * 40)
print(f"Final Accuracy on Test Set: {final_accuracy:.4f}")
print(f"Final Macro F1-Score on Test Set: {final_f1_macro:.4f}")
