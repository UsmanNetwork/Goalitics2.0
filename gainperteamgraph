import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import f1_score, make_scorer

# --- Recreate Data and Preprocessor to ensure feature naming is correct ---

# Assuming your file is accessible in the mounted drive path
# NOTE: Replace this path with the actual path to your file on Google Drive
file_path = '/content/drive/MyDrive/Your_Project_Folder/epl_with_agent_features.csv'
# Since the actual path is unknown, we will use a placeholder and assume the file is loaded
try:
    df = pd.read_csv('epl_with_agent_features.csv')
except FileNotFoundError:
    print("Warning: Using placeholder data. Ensure 'epl_with_agent_features.csv' is in your Colab environment or update the file path.")
    # Fallback to create a DataFrame structure similar to the real one for demonstration
    # You MUST load your actual data in your notebook for this to work correctly.
    data_dict = {
        'HomeTeam': np.random.choice(['Arsenal', 'Chelsea', 'Man Utd'], size=100),
        'AwayTeam': np.random.choice(['Spurs', 'Liverpool', 'Man City'], size=100),
        'FullTimeResult': np.random.choice(['H', 'D', 'A'], size=100),
        'Home_Optimal_Form_Score': np.random.rand(100) * 15,
        'Away_Optimal_Form_Score': np.random.rand(100) * 15,
        'Optimal_Form_Differential': (np.random.rand(100) * 10) - 5,
        'HomeShots': np.random.randint(5, 25, 100),
        'AwayShots': np.random.randint(5, 25, 100),
        'HomeShotsOnTarget': np.random.randint(1, 10, 100),
        'AwayShotsOnTarget': np.random.randint(1, 10, 100),
        'HomeCorners': np.random.randint(1, 15, 100),
        'AwayCorners': np.random.randint(1, 15, 100),
        'HomeFouls': np.random.randint(5, 25, 100),
        'AwayFouls': np.random.randint(5, 25, 100),
        'HomeYellowCards': np.random.randint(0, 5, 100),
        'AwayYellowCards': np.random.randint(0, 5, 100),
        'HomeRedCards': np.random.randint(0, 2, 100),
        'AwayRedCards': np.random.randint(0, 2, 100),
    }
    df = pd.DataFrame(data_dict)


# Target Encoding
le = LabelEncoder()
df['Target'] = le.fit_transform(df['FullTimeResult'])

# Feature Selection
numerical_features = ['HomeShots', 'AwayShots', 'HomeShotsOnTarget', 'AwayShotsOnTarget',
                      'HomeCorners', 'AwayCorners', 'HomeFouls', 'AwayFouls',
                      'HomeYellowCards', 'AwayYellowCards', 'HomeRedCards', 'AwayRedCards',
                      'Home_Optimal_Form_Score', 'Away_Optimal_Form_Score', 'Optimal_Form_Differential']
categorical_features = ['HomeTeam', 'AwayTeam']

X = df[numerical_features + categorical_features]
y = df['Target']

# Time-Series Train-Test Split (80/20 split - needed to correctly fit the final model)
split_point = int(0.8 * len(X))
X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]

# Preprocessing Pipeline Definition
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ],
    remainder='passthrough'
)

# --- RE-DEFINE AND FIT THE BEST MODEL (Same as your GridSearchCV output) ---
best_params = {'classifier__gamma': 0, 'classifier__learning_rate': 0.05, 'classifier__max_depth': 7}

xgb_classifier = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    eval_metric='mlogloss',
    random_state=42,
    n_estimators=100,
    use_label_encoder=False,
    **{k.replace('classifier__', ''): v for k, v in best_params.items()} # Unpack best params
)

smote_sampler = SMOTE(random_state=42)

# Create the final best pipeline
final_best_pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('sampler', smote_sampler),
    ('classifier', xgb_classifier)
])

print("Re-fitting the final best model to extract feature importance...")
final_best_pipeline.fit(X_train, y_train)

# --- 1. Extract Feature Names ---

# Get the list of original column names
feature_names_out = numerical_features

# Get the feature names generated by the OneHotEncoder
one_hot_features = final_best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)

# Combine the two lists of feature names
all_feature_names = list(feature_names_out) + list(one_hot_features)

# --- 2. Extract Feature Importance ---
# XGBoost provides feature importance via .feature_importances_
feature_importances = final_best_pipeline.named_steps['classifier'].feature_importances_

# Create a DataFrame for easy sorting and plotting
importance_df = pd.DataFrame({
    'Feature': all_feature_names,
    'Importance': feature_importances
})

# Filter out the zero-importance features (often from the OneHotEncoder)
importance_df = importance_df[importance_df['Importance'] > 0]

# Sort by importance and select the top 20 features for clear visualization
top_n = 20
importance_df = importance_df.sort_values(by='Importance', ascending=False).head(top_n)

# --- 3. Generate the Plot ---
plt.figure(figsize=(10, 8))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='teal')
plt.xlabel('Feature Importance (Gain)')
plt.title(f'Top {top_n} Most Important Features for Optimized XGBoost Model')
plt.gca().invert_yaxis() # Highest importance at the top
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.savefig('xgboost_feature_importance.png')
print("\nFeature importance plot saved as xgboost_feature_importance.png")
