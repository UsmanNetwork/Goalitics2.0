import pandas as pd
import numpy as np
# New imports for imbalanced learning and XGBoost
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import xgboost as xgb

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.base import BaseEstimator, TransformerMixin # Necessary for the custom LinearRegressionClassifier, but not for XGBoost

# --- Recreate Data and Preprocessor from Deliverable 3 ---

# Load the enriched dataset
df = pd.read_csv('epl_with_agent_features.csv')

# Target Encoding (H=2, D=1, A=0)
le = LabelEncoder()
df['Target'] = le.fit_transform(df['FullTimeResult'])

# Feature Selection
numerical_features = ['HomeShots', 'AwayShots', 'HomeShotsOnTarget', 'AwayShotsOnTarget',
                      'HomeCorners', 'AwayCorners', 'HomeFouls', 'AwayFouls',
                      'HomeYellowCards', 'AwayYellowCards', 'HomeRedCards', 'AwayRedCards',
                      'Home_Optimal_Form_Score', 'Away_Optimal_Form_Score', 'Optimal_Form_Differential']
categorical_features = ['HomeTeam', 'AwayTeam']

X = df[numerical_features + categorical_features]
y = df['Target']

# Time-Series Train-Test Split (80/20 split)
split_point = int(0.8 * len(X))
X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]

# Preprocessing Pipeline Definition (remains the same)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'
)

# --- DELIVERABLE 4: SMOTE + XGBOOST IMPLEMENTATION ---

# 1. Define the Sampler and the Advanced Model
smote_sampler = SMOTE(random_state=42)
xgb_classifier = xgb.XGBClassifier(
    objective='multi:softmax',          # Required for multi-class prediction (H, D, A)
    num_class=3,                        # Three possible outcomes (0, 1, 2)
    eval_metric='mlogloss',             # Evaluation metric for multi-class
    n_estimators=100,
    random_state=42,
    use_label_encoder=False             # Standard practice to disable the deprecated encoder
)

# 2. Create the Integrated imblearn Pipeline (The NEW Structure)
# Sequence: Preprocessor -> SMOTE -> XGBoost Classifier
full_xgb_pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('sampler', smote_sampler),       # <-- SMOTE applied ONLY to X_train here!
    ('classifier', xgb_classifier)    # <-- The final model
])

# 3. Train and Evaluate
print("--- Training XGBoost with SMOTE ---")
full_xgb_pipeline.fit(X_train, y_train)
y_pred = full_xgb_pipeline.predict(X_test)

# Calculate and print metrics (same method as Deliverable 3)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)

print("\n--- XGBoost with SMOTE Performance ---")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (Macro): {precision:.4f}")
print(f"Recall (Macro): {recall:.4f}")
print(f"F1-Score (Macro): {f1:.4f}")

# The goal is to see a significant improvement in the Macro F1-Score (above 0.5095)
